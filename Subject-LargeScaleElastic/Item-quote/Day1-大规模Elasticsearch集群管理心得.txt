https://elasticsearch.cn/article/110

互联网公司主要用于两种应用场景(实时检索与分析)
1.构建业务的搜索功能模块且多是垂直领域的搜索,数量级数十亿级别
2.大规模数据的实时OLAP，经典的如ELKStack,数量级数千亿级别
总结:这两种场景的数据索引和应用访问模式上差异较大，在硬件选型和集群优化方面侧重点也会有所不同

目前我们最大的日志单集群有120个data node，运行于70台物理服务器上。数据规模如下:
	单日索引数据条数600亿，新增索引文件25TB (含一个复制片则为50TB)
	业务高峰期峰值索引速率维持在百万条/秒
	历史数据保留时长根据业务需求制定，从10天 - 90天不等
	集群共3441个索引、17000个分片、数据总量约9300亿, 磁盘总消耗1PB
	Kibana用户600多人, 每日来自Kibana和第三方的API调用共63万次
	查询响应时间百分位 75%:0.160s  90%:1.640s 95%:6.691s 99%:14.0039s

管理工具:
	1.官方提供了ES Puppet Module和Chef Cookbook
	2.Ansible,集群的初始部署，配置批量更改，集群版本升级，重启故障结点都会快捷和安全许多。
	3.第二个必备利器就是sense插件。通过这个插件直接调用集群的restful API，在做集群和索引的状态查看，索引配置更改的时候非常方便。语法提示和自动补全功能更是实用，减少了翻看文档的频率。在Kibana5里面，sense已经成为一个内置的控制台，无需额外安装。

硬件配置:
	我们采用的是32vcoreCPU + 128GB RAM的服务器，磁盘配置大部分服务器是12块4TB SATA机械磁盘做的Raid0，少部分机器是刚上了不久的6块800GB SSD raid0，主要目的是想做冷热数据分离

集群管理
	1.角色分离:
	(1)然而对于一个规模较大，用户较多的集群，master和client在一些极端使用情况下可能会有性能瓶颈甚至内存溢出，从而使得共存的data node故障;
	(2)data node的故障恢复涉及到数据的迁移，对集群资源有一定消耗，容易造成数据写入延迟或者查询减慢
	(3)如果将master和client独立出来，一旦出现问题，重启后几乎是瞬间就恢复的，对用户几乎没有任何影响
	(4)更容易掌握data node资源消耗与写入量和查询量之间的联系，便于做容量管理和规划。

	2.避免过高的并发，包括控制shard数量和threadpool的数量
	(1)分片过多会带来诸多负面影响:过多的并发带来的线程切换造成过多的CPU损耗;过多的小segment会带来非常显著的heap内存消耗

	3.冷热数据最好做分离.
	(1)日志型应用来说，一般是每天建立一个新索引,天的热索引在写入的同时也会有较多的查询
	(2)利用ES可以给结点配置自定义属性的功能，为冷结点加上"boxtype":"weak"的标识，每晚通过维护脚本更新冷数据的索引路由设置index.routing.allocation.{require|include|exclude}，让数据自动向冷结点迁移
	(3)在冷结点上跑3个ES实例，每个分配31GB heap空间，从而可以在一台物理服务器上存储30多TB的索引数据并保持open状态

	4.不同数据量级的shard最好隔离到不同组别的结点.
	(1)自动平衡原理:其一同一索引下的shard尽量分散到不同的结点;其二每个结点上的shard数量尽量接近;其三结点的磁盘有足够的剩余空间
	(2)实际应用中，我们有200多种索引，数据量级差别很大，大的一天几个TB，小的一个月才几个GB，并且每种类型的数据保留时长又千差万别。抛出的问题，就是如何能比较平衡并充分的利用所有节点的资源。 针对这个问题，我们还是通过对结点添加属性标签来做分组，结合index routing控制的方式来做一些精细化的控制。尽量让不同量级的数据使用不同组别的结点，使得每个组内结点上的数据量比较容易自动平衡。

(ES的node是可以增加属性标识的，然后可以根据这些标识进行索引的shard分配设置。 具体参考： https://www.elastic.co/guide/en/elasticsearch/reference/5.4/shard-allocation-filtering.html)

	5.定期做索引的force merge，并且最好是每个shard merge成一个segment
	(1)heap消耗与segment数量也有关系，force merge可以显著降低这种消耗
	(2)就是对于terms aggregation，搜索时无需构造Global Ordinals，可以提升聚合速度。

监控:
	不差钱没空折腾的建议还是买官方的xpack省心
	1.各类Thread pool的使用情况，active/queue/reject可视化出来,判断集群是否有性能瓶颈了，看看业务高峰期各类queue是不是很高，reject是不是经常发生，基本可以做到心里有数
	2.JVM的heap used%以及old GC的频率，如果old GC频率很高，并且多次GC过后heap used%几乎下不来，说明heap压力太大，要考虑扩容了
	3.Segment memory大小和Segment的数量。节点上存放的索引较多的时候，这两个指标就值得关注，要知道segment memory是常驻heap不会被GC回收的，因此当heap压力太大的时候，可以结合这个指标判断是否是因为节点上存放的数据过多，需要扩容。Segement的数量也是比较关键的，如果小的segment非常多，比如有几千，即使segment memory本身不多，但是在搜索线程很多的情况下，依然会吃掉相当多的heap，原因是lucene为每个segment会在thread local里记录状态信息，这块的heap内存开销和(segment数量* thread数量)相关
	4.很有必要记录用户的访问记录。我们只开放了http api给用户，前置了一个nginx做http代理，将用户第三方api的访问记录通过access log全部记录下来。通过分析访问记录，可以在集群出现性能问题时，快速找到问题根源，对于问题排查和性能优化都很有帮助。

